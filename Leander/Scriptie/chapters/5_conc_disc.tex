The focus of this chapter is to interpret the results reported in the previous chapter. After this interpretation a discussion of the results is held. The different architectures are compared to each other as well will the SbA approach be compared to the DSA approach to MSA. These comparisons aim to provide an answer to the research questions.


\section{Results Evaluation}
\subsubsection{Beat Classification Accuracy}
Looking at the absolute performance of the best CNN and LSTM model on classifying a beat, one may conclude that the convolutional model performs better than the long short-term memory model. However, looking at the accuracy of both models on the training and test data, the CNN model drops relatively more in accuracy (15.4\% for the CNN, 6.8\% for the LSTM). This may be an indication of the CNN model being overfit on the training data, therefore reducing its capabilities of classifying beats that the model has not 'seen' before. The possible overfit is unlikely to be a result of too little data since the LSTM did not seem to overfit on the data. The relatively small drop in accuracy for the LSTM model going from the training data to the test data may also indicate that the LSTM managed to capture the overall patterns that define the label of a certain beat, although less accurate for this particular set of beats.

\subsection{Segment Boundary Detection Accuracy}
When looking at the results of the JAMS files with the filtered labels evaluated by the MSAF, we need to take them with a grain of salt. There are a few reasons for this.

\subsubsection{Own ground truth}
Although the models have been trained and evaluated on the ground truth I created, MSAF only evaluates the accuracy of locating the segment boundary locations of each model. The accuracy is therefore not only subject to the pure performance of a model to classify a beat, but also to the performance of the filtering function. Since I primarily focused on the first, the latter received way less to none effort. As described in \autoref{ch:results}, the filtering function was based on the predictions made on SALAMI 1200, and the function was therefore optimized to generate filtered labels, and therefore segment boundaries that would make sense, for this specific song. We see that this indeed is the case, since the trimmed and untrimmed F-measures, with 3 seconds tolerance, on SALAMI 1200 (own ground truth) far exceeds the trimmed and untrimmed 3 second F-measure on the full dataset (own ground truth).

It is, however, interesting that the trimmed and untrimmed 0.5 second F-measure on SALAMI 1200 (own ground truth) is lower than the same F-measure on the full dataset (own ground truth) for both the CNN and LSTM model. We see that both the trimmed and untrimmed F-measures severely drop going from a 3 second tolerance to a 0.5 second tolerance (25\% to 45\%). This applies to both the CNN and LSTM model. Considering that this drop in F-measure is less present on the full dataset, this may indicate that the filtering function is not that bad after all, or that the other songs generally have stronger segment boundaries therefore enabling the models to detect the exact position more precisely. Further research is needed to show this.

\subsubsection{SALAMI ground truth}
I have also evaluated the JAMS files produced by the models on the official SALAMI ground truth. When looking at the F-measures of the models on SALAMI 1200, we see that the CNN model outperforms the LSTM model. However, if we look at the F-measures of the CNN model and LSTM model on the full dataset (SALAMI ground truth), we see that the differences are gone. If any, they are in the advantage of the LSTM. 

\subsection[Own GT. vs SALAMI GT.]{Own ground truth vs SALAMI ground truth}
\label{sec:gt_vs_salami}
As we can see, all F-measures on both SALAMI 1200 and the full dataset drop massively when taking the (official) SALAMI ground truth. This has a few reasons. 

\subsubsection{Unique Label Count}
The most important reason is the amount of unique labels occurring in the SALAMI ground truth and in my own ground truth. As discussed in the \autoref{sec:3.the_data} and showed in \autoref{tab:label_grouping}, I've reduced that amount of unique labels from 26 to 9. I have done this because of the low amount of occurrences of some of the original labels in the dataset (10 labels had around or less that 10 occurrences in the original segments). 

If all 26 unique labels were kept, it may have occurred that some labels were only present in the test data, therefore removing the possibility of the models to learn the patterns corresponding to those labels. It also would've almost tripled the amount of neurons in the output layer of both models. Generally, adding more outputs to a model lowers its accuracy, especially when each output has very similar input (\textit{transition} and \textit{interlude}, \textit{pre-verse} and \textit{verse}). In addition, it would've also almost tripled the size of the ground truth output matrix and increased the amount of weights, greatly slowing down the training and evaluation speed of the models.

As a result of this decision, way less segment boundaries occur in my own ground truth than that they occur in the SALAMI ground truth, thus obviously lowering the accuracy of the model. This is supported by the Normalized Entropy Scores Precision (\textit{So}) values of the CNN and LSTM being 0.479 and 0.474 respectively, whilst the Normalized Entropy Scores Recall (\textit{Su}) values are 0.602 and 0.614 respectively \cite{Lukashevich2008towards}\footnote{If the \textit{So} value is lower than the \textit{Su} value, the model is undersegmenting the data.}. As a side effect, more labels also meant that some segments were less than 4 or 5 beats, and would therefore not be detected by my filtering function. It is thus of relatively high priority to, in follow-up research, come up with a better filtering function.

\subsubsection{Annotator Inconsistency}
The amount of unique labels, and the low occurrence of some of them, may be due to the different annotators that have created the SALAMI ground truth. As mentioned in the introduction, the experience of music differs per person and is i.a. subject to emotion. This means that some annotator may classify one segment as \textit{instrumental}, while another annotator classifies the same segment as \textit{solo}, thereby increasing the amount of unique labels, without there being any musical difference. A consequence of this is that some annotator, judging \textit{instrumental} and \textit{solo} as being different segment, may place more segment boundaries in a piece of music compared to an annotator who judges \textit{instrumental} and \textit{solo} as equal.

\subsubsection{Recurring Segments}
Another reason that my models may under-segment compared to the SALAMI ground truth, is that a human annotator would place a boundary between two repeated choruses. My models would classify most of the beats in these repeated choruses as \textit{chorus}, and the filtering function will therefore see this as one long \textit{chorus} segment.

Also models implementing the Distance-based Approach, without using a repetition-based technique, will face the same problem. This is therefore an important factor to take into future research; how often a repetition of similarly labeled segments occur in music and which methods for MSA can detect this.

\section{Model Comparison}
As is the main research question of this thesis, I want to research the feasibility of machine learning for the segmentation by annotation approach to music structure analysis. 

\subsection{CNN versus LSTM}
To find an answer to this question I have first explored which deep learning architecture, implementing the SbA approach, produces the best results. I have proposed two deep learning architectures, one using convolutional layers, one using long short-term memory units. I have tried multiple hyperparameters to produce, for each architecture, a best model.

As the results showed, the convolutional architecture to produced the best results overall, however the long short-term memory architecture showed some interesting patterns that could mean that with future research, this architecture can become equal to, or even surpass, the CNN architecture in terms of performance. For this thesis however, I will take the convolutional architecture as best implementation of the segmentation by annotation approach to music structure analysis.

\subsection{SbA versus DSA}
To place the feasibility of machine learning within all approaches of music structure analysis, I have done research to the the other general approach to MSA, the distance-based segmentation by annotation approach respectively and to the current state-of-the-art of music structure analysis. \textcite{Jesperthesis} describes an successful attempt at improving the current best implementation to the DSA approach to MSA, using feature fusion. His best performing model produces an 0.5 second F-measure of 0.327. Compared to \textcite{Grill2015music2}, who report a 0.5 second F-measure of 0.508 on a similar but slightly different subset of the SALAMI dataset, this is quite a lot lower. \textcite{Jesperthesis} does however show that the DSA approach has still room for improvement. The smaller and slightly different subset of the SALAMI dataset used can cause the gap to seem bigger than it may be.

\subsubsection{Advantages and Disadvantages}
One big advantage of the segmentation by annotation approach, and therefore the models I introduced, is that both annotation and segmentation is performed within one model. The DSA approach first detects the segment boundaries with one model, and then requires another model to label the data. In previous work, a clustering method was used to group similar segments. Each cluster was then given a capital letter based on the order of occurrence of the segments in the song.

This, however, is still behind the SbA approach, since in the SbA approach the actual segment function is assigned to each segment. Future research has to prove whether each capital letter, assigned via a certain method, always correspond to a segment function. Otherwise, a more complex model is required to assign a function to each segment. If a machine learning model is used, one could argue that using one machine learning model to both annotate and segment a song is more efficient and effective.

An advantage of using DSA over SbA is that a DSA approach model has a higher chance to be applicable on multiple music genres. Although further research has to prove this, the DSA approach detects changes or repeating patterns in music, and uses a quite versatile distance metric for clustering. Since these musical properties are not per definition genre specific, a DSA approach model will be more capable of finding the segment boundaries, while a new SbA model must be created for each specific genre. If the segments need to be labeled, using SbA may then be more effective again, since a specific labeling model then needs to be created anyways.

\subsubsection{Conclusion}
As explained earlier, due to i.a. the different labels and therefore ground truths used, comparing my results to the current reported results is quite difficult. However the results of the convolutional architecture on both the SALAMI 1200 song with the SALAMI ground truth and full dataset with own ground truth, show that a convolutional architecture implementation of the SbA approach has similar performance as the best DSA approach implementation and current state-of-the-art for MSA. Especially seen the early development stage of my proposed architecture.\\

For me, this indicates that there is enough reason to put more effort into exploring the segmentation by annotation approach to music structure analysis and an implementation of that approach using machine learning in particular. I will discuss these efforts and the advantages as well as the disadvantages of this approach in the next chapter.